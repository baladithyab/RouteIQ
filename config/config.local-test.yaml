# LiteLLM + LLMRouter - Local Test Configuration
# Tests all features: A2A, MCP, Routing Strategies, Hot Reload
# Uses AWS Bedrock models via Cross-Region Inference Profiles (us-west-2)

# =============================================================================
# Model List - AWS Bedrock Models (IAM Profile auth)
# All chat models use cross-region inference profiles (us.* or global.*)
# for automatic failover across us-east-1, us-east-2, us-west-2
# =============================================================================
model_list:
  # ---------------------------------------------------------------------------
  # Claude 4.5 (Anthropic via Bedrock - Global Inference Profile)
  # ---------------------------------------------------------------------------
  - model_name: claude-4.5-opus
    litellm_params:
      model: bedrock/global.anthropic.claude-opus-4-5-20251101-v1:0
      aws_region_name: us-west-2
    model_info:
      id: claude-4.5-opus
      mode: chat
      description: "Claude Opus 4.5 - Highest capability, global inference profile"

  # ---------------------------------------------------------------------------
  # Claude 4 Family (Anthropic via Bedrock - Cross-Region Inference)
  # us.* profiles span us-east-1, us-east-2, us-west-2
  # global.* profiles span all available regions
  # ---------------------------------------------------------------------------
  - model_name: claude-4-sonnet
    litellm_params:
      model: bedrock/global.anthropic.claude-sonnet-4-20250514-v1:0
      aws_region_name: us-west-2
    model_info:
      id: claude-4-sonnet
      mode: chat
      description: "Claude Sonnet 4 - Balanced performance, global inference"

  - model_name: claude-4-opus
    litellm_params:
      model: bedrock/us.anthropic.claude-opus-4-20250514-v1:0
      aws_region_name: us-west-2
    model_info:
      id: claude-4-opus
      mode: chat
      description: "Claude Opus 4 - High capability (us-east-1/2, us-west-2)"

  # ---------------------------------------------------------------------------
  # Claude 3.x Family (Anthropic via Bedrock - Cross-Region Inference)
  # ---------------------------------------------------------------------------
  - model_name: claude-3.7-sonnet
    litellm_params:
      model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0
      aws_region_name: us-west-2
    model_info:
      id: claude-3.7-sonnet
      mode: chat
      description: "Claude 3.7 Sonnet - Extended thinking (us-east-1/2, us-west-2)"

  - model_name: claude-3.5-sonnet
    litellm_params:
      model: bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0
      aws_region_name: us-west-2
    model_info:
      id: claude-3.5-sonnet
      mode: chat
      description: "Claude 3.5 Sonnet v2 (us-east-1/2, us-west-2)"

  - model_name: claude-3.5-haiku
    litellm_params:
      model: bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0
      aws_region_name: us-west-2
    model_info:
      id: claude-3.5-haiku
      mode: chat
      description: "Claude 3.5 Haiku - Fast and cost-effective (us-east-1/2, us-west-2)"

  # ---------------------------------------------------------------------------
  # Amazon Nova Family (Cross-Region Inference Profiles)
  # ---------------------------------------------------------------------------
  - model_name: nova-premier
    litellm_params:
      model: bedrock/us.amazon.nova-premier-v1:0
      aws_region_name: us-west-2
    model_info:
      id: nova-premier
      mode: chat
      description: "Nova Premier - Most capable Nova model (us-east-1/2, us-west-2)"

  - model_name: nova-pro
    litellm_params:
      model: bedrock/us.amazon.nova-pro-v1:0
      aws_region_name: us-west-2
    model_info:
      id: nova-pro
      mode: chat
      description: "Nova Pro - Highly capable multimodal (us-east-1/2, us-west-2)"

  - model_name: nova-micro
    litellm_params:
      model: bedrock/us.amazon.nova-micro-v1:0
      aws_region_name: us-west-2
    model_info:
      id: nova-micro
      mode: chat
      description: "Nova Micro - Ultra-fast text-only (us-east-1/2, us-west-2)"

  - model_name: nova-2-lite
    litellm_params:
      model: bedrock/us.amazon.nova-2-lite-v1:0
      aws_region_name: us-west-2
    model_info:
      id: nova-2-lite
      mode: chat
      description: "Nova 2 Lite - Fast multimodal (us-east-1/2, us-west-2)"

  # ---------------------------------------------------------------------------
  # Meta Llama 4 Family (Cross-Region Inference Profiles)
  # ---------------------------------------------------------------------------
  - model_name: llama-4-maverick
    litellm_params:
      model: bedrock/us.meta.llama4-maverick-17b-instruct-v1:0
      aws_region_name: us-west-2
    model_info:
      id: llama-4-maverick
      mode: chat
      description: "Llama 4 Maverick 17B (us-east-1/2, us-west-2)"

  - model_name: llama-4-scout
    litellm_params:
      model: bedrock/us.meta.llama4-scout-17b-instruct-v1:0
      aws_region_name: us-west-2
    model_info:
      id: llama-4-scout
      mode: chat
      description: "Llama 4 Scout 17B (us-east-1/2, us-west-2)"

  # ---------------------------------------------------------------------------
  # DeepSeek (Cross-Region Inference Profile)
  # ---------------------------------------------------------------------------
  - model_name: deepseek-r1
    litellm_params:
      model: bedrock/us.deepseek.r1-v1:0
      aws_region_name: us-west-2
    model_info:
      id: deepseek-r1
      mode: chat
      description: "DeepSeek-R1 - Reasoning model (us-east-1/2, us-west-2)"

  # ---------------------------------------------------------------------------
  # Embedding Models (on-demand, us-west-2 region-local)
  # ---------------------------------------------------------------------------
  - model_name: titan-embed-text-v2
    litellm_params:
      model: bedrock/amazon.titan-embed-text-v2:0
      aws_region_name: us-west-2
    model_info:
      id: titan-embed-text-v2
      mode: embedding
      description: "Titan Text Embeddings V2 - High quality text embeddings"

  - model_name: titan-embed-image-v1
    litellm_params:
      model: bedrock/amazon.titan-embed-image-v1
      aws_region_name: us-west-2
    model_info:
      id: titan-embed-image-v1
      mode: embedding
      description: "Titan Multimodal Embeddings - Text and image embeddings"

  # ---------------------------------------------------------------------------
  # Image Generation Models (on-demand, us-west-2 region-local)
  # ---------------------------------------------------------------------------
  - model_name: titan-image-generator
    litellm_params:
      model: bedrock/amazon.titan-image-generator-v2:0
      aws_region_name: us-west-2
    model_info:
      id: titan-image-generator
      mode: image_generation
      description: "Titan Image Generator V2 - Versatile image generation"

# =============================================================================
# Router Settings - Test Multiple Strategies
# =============================================================================
router_settings:
  # Use simple-shuffle for local testing (no trained model needed)
  # Switch to llmrouter-knn when trained model is available in /app/models/
  routing_strategy: simple-shuffle

  # LLMRouter strategy arguments (for when llmrouter-knn is enabled)
  # routing_strategy_args:
  #   model_path: /app/models/knn_router.pkl
  #   llm_data_path: /app/config/llm_candidates.json
  #   hot_reload: true
  #   reload_interval: 10
  #   embedding_model: sentence-transformers/all-MiniLM-L6-v2
  #   embedding_device: cpu
    #   model-label-2: nova-pro

  num_retries: 2
  retry_after: 5
  timeout: 300
  cache_responses: true

# =============================================================================
# General Settings
# =============================================================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  # Enable database for full feature testing
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true
  # Enable detailed logging for testing
  # alerting: ["slack"]

# =============================================================================
# LiteLLM Settings
# =============================================================================
litellm_settings:
  cache: true
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST
    port: os.environ/REDIS_PORT
    ttl: 300
  set_verbose: true
  drop_params: true
  # OpenTelemetry callbacks for Jaeger tracing
  success_callback: ["otel"]
  failure_callback: ["otel"]

# =============================================================================
# Observability Settings - Jaeger & MLflow
# =============================================================================
environment_variables:
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://jaeger:4318"
  OTEL_SERVICE_NAME: "litellm-gateway"
  OTEL_TRACES_EXPORTER: "otlp"
  MLFLOW_TRACKING_URI: "http://mlflow:5050"

# =============================================================================
# MCP Servers Configuration
# =============================================================================
# MCP servers can be configured via API when MCP_GATEWAY_ENABLED=true
# The following servers are available via stdio transport in the mcp-proxy container:
#
# Available MCP Servers (stdio-based, no API keys required):
# - @anthropic-ai/server-filesystem: File system access for reading workspace files
# - @anthropic-ai/server-memory: Simple key-value memory for agents
#
# To add HTTP-based MCP servers dynamically:
# POST /mcp/servers with body:
# {
#   "server_id": "my-mcp-server",
#   "name": "My MCP Server",
#   "url": "http://localhost:8080/mcp",
#   "transport": "streamable_http"
# }

# =============================================================================
# A2A Agents - Test Configuration
# =============================================================================
# A2A agents are managed via API when A2A_GATEWAY_ENABLED=true
# Example agents that can be added via API:
# POST /a2a/agents with body:
# {
#   "agent_id": "my-agent",
#   "name": "My Agent",
#   "description": "A test agent",
#   "url": "http://localhost:9000/a2a",
#   "capabilities": ["chat", "code"]
# }

# =============================================================================
# Service Endpoints Reference (for testing)
# =============================================================================
# LiteLLM Gateway:     http://localhost:4010
# Jaeger UI:           http://localhost:16686
# MLflow UI:           http://localhost:5050
# MinIO Console:       http://localhost:9001 (admin: minioadmin/minioadmin)
# MinIO API:           http://localhost:9000
# MCP Proxy:           http://localhost:3100-3103 (for MCP server access)
