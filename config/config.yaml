# LiteLLM + LLMRouter Configuration
# =============================================================================
# This is the main configuration file for the LiteLLM gateway with LLMRouter
# intelligent routing strategies.

# =============================================================================
# Model List - Define your LLM providers and models
# =============================================================================
model_list:
  # Anthropic Models (Direct API)
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-3-sonnet
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY

  # AWS Bedrock Models (matches trained router models)
  - model_name: claude-haiku
    litellm_params:
      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0
      aws_region_name: os.environ/AWS_REGION

  - model_name: claude-sonnet
    litellm_params:
      model: bedrock/anthropic.claude-3-sonnet-20240229-v1:0
      aws_region_name: os.environ/AWS_REGION

  - model_name: claude-opus
    litellm_params:
      model: bedrock/anthropic.claude-3-opus-20240229-v1:0
      aws_region_name: os.environ/AWS_REGION

# =============================================================================
# Router Settings - LLMRouter Integration
# =============================================================================
router_settings:
  # Routing strategy options:
  # LiteLLM built-in: simple-shuffle, least-busy, latency-based-routing,
  #                   cost-based-routing, usage-based-routing
  # LLMRouter ML-based: llmrouter-knn, llmrouter-svm, llmrouter-mlp,
  #                     llmrouter-mf, llmrouter-elo, llmrouter-hybrid,
  #                     llmrouter-custom
  # NOTE: llmrouter-* strategies require registration with LiteLLM - see ISSUE#1
  routing_strategy: llmrouter-knn

  # LLMRouter strategy arguments (used when routing_strategy starts with 'llmrouter-')
  routing_strategy_args:
    # Path to trained model directory (trained via examples/mlops pipeline)
    model_path: /app/models/knn_router
    # Path to LLM candidates JSON
    llm_data_path: /app/config/llm_candidates.json
    # Enable hot-reloading of model
    hot_reload: true
    # How often to check for model updates (seconds)
    reload_interval: 300
    # S3 settings for remote model loading (optional)
    # model_s3_bucket: my-bucket
    # model_s3_key: models/knn_router/

  # Retry/fallback settings
  num_retries: 2
  retry_after: 5
  timeout: 600

  # Enable routing caching
  cache_responses: true

# =============================================================================
# Centroid Routing (Zero-Config Intelligent Routing)
# =============================================================================
# NadirClaw-inspired centroid-based routing that works immediately without
# ML model training. Uses pre-computed centroid vectors for ~2ms prompt
# classification into complexity tiers (simple vs complex).
#
# Centroid routing is enabled by default as a fallback when no ML strategies
# (llmrouter-*) are configured. It sits in the progressive enhancement chain:
#   Pipeline strategies → Centroid (~2ms) → Random fallback
#
# To use centroid routing as the primary strategy:
# routing_strategy: nadirclaw-centroid
# routing_strategy_args:
#   centroid_dir: models/centroids       # Directory with .npy centroid files
#   confidence_threshold: 0.06           # Min cosine distance for classification
#   profile: auto                        # Routing profile (see below)
#   session_ttl: 1800                    # Session cache TTL in seconds
#   tier_mapping:                        # Map tiers to model deployments
#     simple:
#       - gpt-4o-mini
#       - claude-haiku
#     complex:
#       - gpt-4o
#       - claude-sonnet
#
# Routing profiles:
#   auto      - Automatic tier selection based on prompt complexity
#   eco       - Prefer cheaper models, only upgrade for complex prompts
#   premium   - Always use high-quality models regardless of complexity
#   free      - Route to free-tier models only
#   reasoning - Optimized for math/logic tasks with reasoning detection

# =============================================================================
# General Settings
# =============================================================================
general_settings:
  # Master key for admin access
  master_key: os.environ/LITELLM_MASTER_KEY

  # Database for persistence
  database_url: os.environ/DATABASE_URL

  # Store models in DB
  store_model_in_db: true

# =============================================================================
# LiteLLM Settings
# =============================================================================
litellm_settings:
  # Enable response caching
  cache: true
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST
    port: os.environ/REDIS_PORT
    ttl: 3600

  # Logging
  set_verbose: false

  # Cost tracking callbacks (optional)
  # Uncomment and configure to enable Langfuse LLM observability:
  # Requires: LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY env vars
  # Optional: LANGFUSE_HOST (defaults to https://cloud.langfuse.com)
  # success_callback: ["langfuse"]
  # failure_callback: ["langfuse"]

  # Rate limiting (optional)
  # max_budget: 100
  # budget_duration: 30d

# =============================================================================
# MCP Servers - Model Context Protocol (optional)
# =============================================================================
# Uncomment and configure to enable MCP tools
# mcp_servers:
#   # HTTP-based MCP server
#   my_mcp_server:
#     url: "http://localhost:8080/mcp"
#     transport: "streamable_http"  # or "sse", "stdio"
#
#   # OpenAPI-to-MCP conversion
#   petstore_api:
#     url: "https://petstore.swagger.io/v2"
#     spec_path: "/path/to/openapi.json"
#     auth_type: "none"  # or "api_key", "bearer_token", "oauth2"

# =============================================================================
# A2A Agents - Agent-to-Agent Protocol (optional)
# =============================================================================
# Agents are typically added via the UI or API, but can also be pre-configured
# See docs: https://docs.litellm.ai/docs/a2a

# =============================================================================
# Environment Variables Reference
# =============================================================================
# Required:
#   - LITELLM_MASTER_KEY: Master API key for admin access
#   - OPENAI_API_KEY: OpenAI API key
#   - ANTHROPIC_API_KEY: Anthropic API key
#
# Optional:
#   - DATABASE_URL: PostgreSQL connection string
#   - REDIS_HOST: Redis host for caching
#   - REDIS_PORT: Redis port (default: 6379)
#   - AZURE_API_KEY: Azure OpenAI API key
#   - AZURE_API_BASE: Azure OpenAI endpoint
#
# Gateway Features:
#   - A2A_GATEWAY_ENABLED: Enable A2A agent gateway (default: false)
#   - MCP_GATEWAY_ENABLED: Enable MCP server gateway (default: false)
#   - STORE_MODEL_IN_DB: Store models/MCPs in database (default: false)
#
# Config Sync & Hot Reload:
#   - CONFIG_S3_BUCKET: S3 bucket for config sync
#   - CONFIG_S3_KEY: S3 key for config file
#   - CONFIG_HOT_RELOAD: Enable hot reload on config changes (default: false)
#   - CONFIG_SYNC_INTERVAL: Sync interval in seconds (default: 60)
#
# Centroid Routing:
#   - ROUTEIQ_CENTROID_ROUTING: Enable centroid routing fallback (default: true)
#   - ROUTEIQ_ROUTING_PROFILE: Default routing profile: auto/eco/premium/free/reasoning (default: auto)
#   - ROUTEIQ_CENTROID_WARMUP: Pre-warm centroid classifier at startup (default: false)
#   - ROUTEIQ_CENTROID_DIR: Directory for centroid .npy files (default: models/centroids)
#   - ROUTEIQ_CONFIDENCE_THRESHOLD: Centroid classification confidence threshold (default: 0.06)
