# RouteIQ Gateway Environment Variables
# Copy this file to .env and fill in your values
#
# SECURITY: Generate secure keys with: openssl rand -hex 32
# NEVER commit real credentials to version control!
#
# For Kubernetes deployments, these values are typically provided via:
# - ConfigMaps (for non-sensitive config)
# - Secrets (for credentials)
# - External secret managers (e.g., AWS Secrets Manager, HashiCorp Vault)

# =============================================================================
# Required Settings (MUST be set before deployment)
# =============================================================================

# Master API key for admin access
# REQUIRED - Generate with: openssl rand -hex 32
# WARNING: This is a placeholder - DO NOT use in production!
LITELLM_MASTER_KEY=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_32

# =============================================================================
# Admin API Keys for Control-Plane Operations
# =============================================================================
# These keys are required to access control-plane endpoints:
# - POST /router/reload, POST /config/reload (hot reload)
# - POST/PUT/DELETE /llmrouter/mcp/servers/* (MCP server management)
# - POST /llmrouter/mcp/tools/call (MCP tool invocation)
# - POST/DELETE /a2a/agents (A2A agent registration)
#
# Generate with: openssl rand -hex 32
# Comma-separated for multiple keys
ADMIN_API_KEYS=

# Legacy single-key support (prefer ADMIN_API_KEYS)
# ADMIN_API_KEY=

# Set to "false" to disable admin auth (NOT recommended for production)
# When disabled, control-plane endpoints fall back to user API key auth only
# Default: true (admin auth enabled)
# ADMIN_AUTH_ENABLED=true

# =============================================================================
# LLM Provider API Keys
# =============================================================================

# OpenAI (replace with your actual key)
OPENAI_API_KEY=

# Anthropic (replace with your actual key)
ANTHROPIC_API_KEY=

# Azure OpenAI
AZURE_API_KEY=
AZURE_API_BASE=

# Google Vertex AI
GOOGLE_APPLICATION_CREDENTIALS=

# AWS Bedrock (uses IAM roles in K8s via IRSA/Pod Identity)
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=us-east-1

# =============================================================================
# Database (PostgreSQL) - Required for HA/K8s
# =============================================================================
# For Kubernetes, use a managed PostgreSQL service (RDS, Cloud SQL, etc.)
# or deploy a PostgreSQL StatefulSet with persistent volumes.

POSTGRES_USER=litellm
# REQUIRED for HA - Generate with: openssl rand -hex 16
# WARNING: This is a placeholder - DO NOT use in production!
POSTGRES_PASSWORD=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_16
POSTGRES_DB=litellm

# Full connection string - typically constructed from above or provided by K8s Secret
# Format: postgresql://USER:PASSWORD@HOST:PORT/DATABASE
DATABASE_URL=postgresql://litellm:YOUR_POSTGRES_PASSWORD@postgres:5432/litellm

# Store LiteLLM models/config in database for multi-replica consistency
# Set to 'true' for Kubernetes deployments
STORE_MODEL_IN_DB=false

# =============================================================================
# Redis - Required for caching and rate limiting in HA/K8s
# =============================================================================
# For Kubernetes, use a managed Redis service (ElastiCache, MemoryStore, etc.)
# or deploy Redis with proper persistence.

REDIS_HOST=redis
REDIS_PORT=6379
# REDIS_PASSWORD=  # Uncomment if Redis requires authentication

# =============================================================================
# Object Storage Configuration Sync (S3/GCS)
# =============================================================================
# For K8s deployments, sync config from object storage instead of ConfigMaps
# for dynamic updates without pod restarts.

# Config from S3
CONFIG_S3_BUCKET=
CONFIG_S3_KEY=configs/config.yaml

# Config from GCS (alternative to S3)
CONFIG_GCS_BUCKET=
CONFIG_GCS_KEY=configs/config.yaml

# Hot reload and sync settings
CONFIG_HOT_RELOAD=false
CONFIG_SYNC_ENABLED=true
CONFIG_SYNC_INTERVAL=60

# =============================================================================
# Config Sync Leader Election (HA Deployments)
# =============================================================================
# In HA deployments with multiple replicas, leader election ensures only one
# replica performs config sync at a time, avoiding thundering herd and conflicts.
#
# Uses a database-backed lease lock (requires DATABASE_URL to be configured).
# Without DATABASE_URL, leader election is disabled and all replicas sync.

# Enable/disable leader election for config sync
# Default: true if DATABASE_URL is set (HA mode), false otherwise
# LLMROUTER_CONFIG_SYNC_LEADER_ELECTION_ENABLED=true

# Lease duration in seconds (how long a leader holds the lock)
# Default: 30
# LLMROUTER_CONFIG_SYNC_LEASE_SECONDS=30

# How often to renew the lease in seconds (should be < lease_seconds / 3)
# Default: 10
# LLMROUTER_CONFIG_SYNC_RENEW_INTERVAL_SECONDS=10

# Lock name (allows multiple independent locks if needed)
# Default: config_sync
# LLMROUTER_CONFIG_SYNC_LOCK_NAME=config_sync

# Models from S3 (for ML-based routing strategies)
LLMROUTER_MODEL_S3_BUCKET=
LLMROUTER_MODEL_S3_KEY=models/router.pt

# AWS Credentials for S3 (if not using IAM roles/IRSA)
# In K8s, prefer IRSA (EKS) or Workload Identity (GKE) over static credentials
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# =============================================================================
# Gateway Feature Flags
# =============================================================================

# Enable MCP (Model Context Protocol) gateway
MCP_GATEWAY_ENABLED=false

# Enable A2A (Agent-to-Agent) gateway
A2A_GATEWAY_ENABLED=false

# MCP HA sync via Redis (for multi-replica K8s deployments)
MCP_HA_SYNC_ENABLED=false

# Enable MCP remote tool invocation (SECURITY SENSITIVE)
# When false (default): POST /llmrouter/mcp/tools/call returns 501 Not Implemented
# When true: Gateway makes outbound HTTP calls to registered MCP servers
# CAUTION: Only enable in trusted environments with proper SSRF allowlists configured
LLMROUTER_ENABLE_MCP_TOOL_INVOCATION=false

# =============================================================================
# Plugin Routing Strategy (TG-IMPL-C)
# =============================================================================
# Controls whether RouteIQ uses LiteLLM's official CustomRoutingStrategyBase
# plugin API instead of the legacy monkey-patch approach.

# Use official LiteLLM plugin API instead of monkey-patch (default: true)
# When true: RouteIQRoutingStrategy is installed via CustomRoutingStrategyBase
# When false: Legacy monkey-patch mode (routing_strategy_patch.py)
ROUTEIQ_USE_PLUGIN_STRATEGY=true

# Override routing strategy name (e.g., llmrouter-knn, llmrouter-mlp)
# Auto-detected from config router_settings.routing_strategy if not set
# ROUTEIQ_ROUTING_STRATEGY=

# Number of uvicorn workers (multi-worker requires plugin strategy mode)
# Default: 1 (legacy monkey-patch mode forces 1 worker regardless)
ROUTEIQ_WORKERS=1

# =============================================================================
# Centroid Routing (NadirClaw-Inspired Zero-Config Routing)
# =============================================================================
# Intelligent prompt classification using pre-computed centroid vectors (~2ms).
# Works immediately without ML model training. Enabled by default as a fallback
# when no pipeline ML strategies (llmrouter-*) are configured.
# Progressive enhancement chain: Pipeline → Centroid → Random fallback

# Enable/disable centroid routing fallback
# Default: true
ROUTEIQ_CENTROID_ROUTING=true

# Default routing profile: auto, eco, premium, free, reasoning
# - auto: Automatic tier selection based on prompt complexity
# - eco: Prefer cheaper models, upgrade only for complex prompts
# - premium: Always use high-quality models
# - free: Route to free-tier models only
# - reasoning: Optimized for math/logic with reasoning detection
# Default: auto
ROUTEIQ_ROUTING_PROFILE=auto

# Pre-warm centroid classifier at startup (loads embeddings into memory)
# Default: false
ROUTEIQ_CENTROID_WARMUP=false

# =============================================================================
# Admin UI
# =============================================================================
# Enable Admin UI (serves React SPA at /ui/)
# Requires ui/dist/ to be built: cd ui && npm run build
# Default: false
ROUTEIQ_ADMIN_UI_ENABLED=false

# Directory containing centroid .npy files (pre-trained vectors)
# Default: models/centroids
# ROUTEIQ_CENTROID_DIR=models/centroids

# Centroid classification confidence threshold (cosine distance)
# Lower values = stricter classification; prompts below threshold use fallback
# Default: 0.06
# ROUTEIQ_CONFIDENCE_THRESHOLD=0.06

# =============================================================================
# SSRF Protection (Outbound URL Security)
# =============================================================================
# Controls validation of outbound URLs for MCP servers and A2A agents.
# By default, private IPs are BLOCKED (secure-by-default / fail-closed).

# Allow all private IP ranges (10.x, 172.16.x, 192.168.x)
# Default: false (blocked) - set to true only for development/testing
LLMROUTER_ALLOW_PRIVATE_IPS=false

# Allowlist specific hosts/domains (comma-separated)
# Supports:
# - Exact match: "myserver.internal"
# - Suffix match: ".trusted.com" matches "api.trusted.com", "mcp.trusted.com"
# Example: LLMROUTER_SSRF_ALLOWLIST_HOSTS=mcp.internal,.trusted-corp.com
LLMROUTER_SSRF_ALLOWLIST_HOSTS=

# Allowlist specific IP ranges in CIDR notation (comma-separated)
# Use this to allow specific private IP ranges for internal services
# Example: LLMROUTER_SSRF_ALLOWLIST_CIDRS=10.100.0.0/16,192.168.50.0/24
LLMROUTER_SSRF_ALLOWLIST_CIDRs=

# New canonical names (preferred over legacy names above):
# LLMROUTER_OUTBOUND_ALLOW_PRIVATE - same as LLMROUTER_ALLOW_PRIVATE_IPS (default: false)
# LLMROUTER_OUTBOUND_HOST_ALLOWLIST - same as LLMROUTER_SSRF_ALLOWLIST_HOSTS
# LLMROUTER_OUTBOUND_CIDR_ALLOWLIST - same as LLMROUTER_SSRF_ALLOWLIST_CIDRs

# Allowlist full URL prefixes that bypass all SSRF checks (comma-separated)
# Use for specific trusted endpoints, e.g. internal MCP servers
# Example: LLMROUTER_OUTBOUND_URL_ALLOWLIST=https://mcp.internal:8443/,https://tools.corp.com/
# LLMROUTER_OUTBOUND_URL_ALLOWLIST=

# -----------------------------------------------------------------------------
# SSRF DNS Resolution (Advanced)
# -----------------------------------------------------------------------------

# Use synchronous DNS resolution instead of async (rollback flag)
# Default: false (use async DNS)
# LLMROUTER_SSRF_USE_SYNC_DNS=false

# DNS resolution timeout in seconds
# Default: 5.0
# LLMROUTER_SSRF_DNS_TIMEOUT=5.0

# DNS cache TTL in seconds (how long resolved addresses are cached)
# Default: 60
# LLMROUTER_SSRF_DNS_CACHE_TTL=60

# Maximum entries in the DNS resolution cache
# Default: 1000
# LLMROUTER_SSRF_DNS_CACHE_SIZE=1000

# =============================================================================
# Redis Advanced Settings
# =============================================================================
# Additional Redis configuration beyond REDIS_HOST / REDIS_PORT / REDIS_PASSWORD.

# Enable TLS/SSL for Redis connections (used in health checks)
# Default: false
# Set to true when connecting to Redis over TLS (e.g., AWS ElastiCache in-transit encryption)
# REDIS_SSL=false

# =============================================================================
# Semantic Cache Plugin (gateway/plugins/cache_plugin.py)
# =============================================================================
# Two-tier LLM response caching: L1 in-memory + L2 Redis (optional).
# Requires the cache plugin to be loaded in config.

# Enable/disable the semantic cache plugin
# Default: false
# CACHE_ENABLED=false

# Enable semantic (embedding-based) matching in addition to exact-key matching
# Default: false
# CACHE_SEMANTIC_ENABLED=false

# Default TTL for cached responses in seconds
# Default: 3600
# CACHE_TTL_SECONDS=3600

# Maximum entries in the L1 in-memory LRU cache
# Default: 1000
# CACHE_L1_MAX_SIZE=1000

# Cosine similarity threshold for semantic cache hits (0.0-1.0)
# Default: 0.95
# CACHE_SIMILARITY_THRESHOLD=0.95

# Redis URL for the L2 (shared) cache tier
# If unset, only L1 in-memory cache is used
# Format: redis://[:password@]host:port/db
# CACHE_REDIS_URL=

# Sentence-transformer model for semantic embeddings
# Default: all-MiniLM-L6-v2
# CACHE_EMBEDDING_MODEL=all-MiniLM-L6-v2

# Maximum temperature for cacheable requests (higher temperatures are too random to cache)
# Default: 0.1
# CACHE_MAX_TEMPERATURE=0.1

# =============================================================================
# Routing Telemetry
# =============================================================================

# Enable the router decision callback for routing telemetry (TG4.1)
# Emits OTel span attributes (router.strategy, etc.) for all routing decisions
# Default: true
# LLMROUTER_ROUTER_CALLBACK_ENABLED=true

# =============================================================================
# Resilience / Backpressure Settings
# =============================================================================
# Controls gateway-level resilience primitives for load shedding and graceful shutdown.
# By default, backpressure is DISABLED (no concurrency limit) for non-breaking behavior.

# Maximum concurrent requests before returning 503 (load shedding)
# Default: 0 (disabled - no concurrency limit)
# When set: Requests over this limit receive HTTP 503 with JSON body:
#   {"error": "over_capacity", "message": "Server is at capacity, please retry later"}
# Health endpoints (/_health/live, /_health/ready) are always excluded from limiting.
# Example: ROUTEIQ_MAX_CONCURRENT_REQUESTS=100
ROUTEIQ_MAX_CONCURRENT_REQUESTS=0

# Drain timeout in seconds for graceful shutdown
# Default: 30
# On shutdown: Server waits up to this many seconds for in-flight requests to complete.
# After timeout, shutdown proceeds even if requests are still active.
# Example: ROUTEIQ_DRAIN_TIMEOUT_SECONDS=60
ROUTEIQ_DRAIN_TIMEOUT_SECONDS=30

# Additional paths to exclude from backpressure limiting (comma-separated)
# Default paths always excluded: /_health/live, /_health/ready, /health/*, etc.
# Example: ROUTEIQ_BACKPRESSURE_EXCLUDED_PATHS=/metrics,/internal/status
ROUTEIQ_BACKPRESSURE_EXCLUDED_PATHS=

# -----------------------------------------------------------------------------
# Circuit Breaker Per-Provider Overrides
# -----------------------------------------------------------------------------
# Circuit breakers protect external service calls (Redis, DB, providers).
# Global defaults apply to all breakers; per-provider overrides use the pattern:
#   ROUTEIQ_CB_{PROVIDER}_FAILURE_THRESHOLD, ROUTEIQ_CB_{PROVIDER}_SUCCESS_THRESHOLD, etc.
# Falls back to ROUTEIQ_CB_FAILURE_THRESHOLD (no provider prefix) then to compiled defaults.

# Global defaults (apply when no per-provider override is set):
# ROUTEIQ_CB_FAILURE_THRESHOLD=5       # Failures before circuit opens
# ROUTEIQ_CB_SUCCESS_THRESHOLD=2       # Successes in half-open before closing
# ROUTEIQ_CB_TIMEOUT_SECONDS=30        # Seconds before attempting recovery (half-open)
# ROUTEIQ_CB_WINDOW_SECONDS=60         # Sliding window for failure tracking

# Per-provider examples (PROVIDER is uppercased):
# ROUTEIQ_CB_OPENAI_FAILURE_THRESHOLD=10
# ROUTEIQ_CB_OPENAI_TIMEOUT_SECONDS=60
# ROUTEIQ_CB_ANTHROPIC_FAILURE_THRESHOLD=8
# ROUTEIQ_CB_BEDROCK_FAILURE_THRESHOLD=5
# ROUTEIQ_CB_REDIS_FAILURE_THRESHOLD=3

# =============================================================================
# OpenTelemetry (OTEL) Observability
# =============================================================================
# For K8s, typically point to an OTEL Collector sidecar or service.

# OTEL Collector endpoint (gRPC)
# Examples:
#   - Sidecar: http://localhost:4317
#   - Service: http://otel-collector.observability:4317
OTEL_EXPORTER_OTLP_ENDPOINT=

# Service name for traces/metrics
OTEL_SERVICE_NAME=litellm-gateway

# Exporter configuration (set to 'otlp' when endpoint is configured)
OTEL_TRACES_EXPORTER=none
OTEL_METRICS_EXPORTER=none
OTEL_LOGS_EXPORTER=none

# Enable OTEL integration
OTEL_ENABLED=true

# -----------------------------------------------------------------------------
# Trace Sampling Configuration
# -----------------------------------------------------------------------------
# Control trace sampling to reduce volume in high-traffic production environments.
# Two approaches are supported:
#
# Option 1: Standard OTEL environment variables (recommended for compliance)
#   OTEL_TRACES_SAMPLER - Sampler type:
#     - "always_on": Sample all traces (default, 100%)
#     - "always_off": Sample no traces (0%)
#     - "traceidratio": Sample based on OTEL_TRACES_SAMPLER_ARG
#     - "parentbased_always_on": Parent-based with always_on root
#     - "parentbased_always_off": Parent-based with always_off root
#     - "parentbased_traceidratio": Parent-based with ratio root (recommended)
#   OTEL_TRACES_SAMPLER_ARG - Ratio for ratio-based samplers (0.0-1.0)
#
# Option 2: Simple rate-based sampling (convenience)
#   LLMROUTER_OTEL_SAMPLE_RATE - Sampling rate 0.0-1.0 (uses parentbased_traceidratio)
#
# Examples:
#   # Sample 10% of traces (high traffic production)
#   OTEL_TRACES_SAMPLER=parentbased_traceidratio
#   OTEL_TRACES_SAMPLER_ARG=0.1
#
#   # Or equivalently using the convenience variable:
#   LLMROUTER_OTEL_SAMPLE_RATE=0.1
#
#   # Sample all traces (development/debugging)
#   OTEL_TRACES_SAMPLER=always_on
#
OTEL_TRACES_SAMPLER=
OTEL_TRACES_SAMPLER_ARG=
LLMROUTER_OTEL_SAMPLE_RATE=

# -----------------------------------------------------------------------------
# Multiprocess Metrics (gunicorn/uvicorn workers)
# -----------------------------------------------------------------------------
# When running with multiple worker processes (gunicorn, uvicorn --workers),
# metrics require special handling to avoid duplicates or data loss.
#
# Recommended approach: Use an OTEL Collector as aggregation point
#   - Each worker sends metrics via OTLP to the collector
#   - Collector aggregates and exports to your backend (Prometheus, etc.)
#   - Set OTEL_EXPORTER_OTLP_ENDPOINT and ensure workers don't share state
#
# For Prometheus pull-based metrics in multiprocess mode:
#   - Set PROMETHEUS_MULTIPROC_DIR to a shared directory
#   - Mount tmpfs or emptyDir for performance
#   - Example: PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_multiproc
#
# See docs/observability.md for detailed multiprocess configuration guides.
#
# PROMETHEUS_MULTIPROC_DIR=

# =============================================================================
# LLMRouter Settings
# =============================================================================

LLMROUTER_HOT_RELOAD=true
LLMROUTER_RELOAD_INTERVAL=300

# =============================================================================
# MLOps (for training setup - not typically needed in production K8s)
# =============================================================================

MLFLOW_TRACKING_URI=http://mlflow:5000
MLFLOW_ARTIFACT_BUCKET=llmrouter-artifacts
WANDB_API_KEY=
HF_TOKEN=

# MinIO (local S3-compatible storage for development)
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# Jupyter
JUPYTER_TOKEN=llmrouter

# =============================================================================
# RouteIQ Key Prefix & Leader Migrations (CDK / HA Deployments)
# =============================================================================

# Custom prefix for RouteIQ-generated API keys
# Default: sk-riq-
# ROUTEIQ_KEY_PREFIX=sk-riq-

# Enable leader-election-based database migrations on startup
# When true, only the HA leader runs schema migrations (prevents race conditions)
# Requires DATABASE_URL and Redis-based leader election to be configured
# Default: false
# ROUTEIQ_LEADER_MIGRATIONS=false

# =============================================================================
# Kubernetes-Specific Notes
# =============================================================================
#
# Database Migrations:
#   - Do NOT run migrations on every replica (set LITELLM_RUN_DB_MIGRATIONS=false)
#   - Use a separate init container or Job for migrations
#   - Example: kubectl create job db-migrate --from=cronjob/litellm-migrate
#
# Health Probes:
#   - Liveness: /_health/live (no external deps, fast)
#   - Readiness: /_health/ready (checks DB/Redis if configured)
#   - LiteLLM native: /health/liveliness, /health/readiness (may be auth-protected)
#
# Network Policies:
#   - Egress to LLM providers (OpenAI, Anthropic, etc.)
#   - Egress to PostgreSQL and Redis
#   - Egress to S3/GCS for config sync
#   - Egress to MCP servers (if MCP_GATEWAY_ENABLED=true)
#   - Egress to A2A agent URLs (if A2A_GATEWAY_ENABLED=true)
#   - Egress to OTEL Collector
#
# Resource Recommendations:
#   - Memory: 512Mi-2Gi depending on traffic
#   - CPU: 500m-2000m depending on traffic
#   - Consider HPA based on CPU/memory or custom metrics
#
# Security Context (recommended):
#   securityContext:
#     runAsNonRoot: true
#     runAsUser: 1000
#     readOnlyRootFilesystem: true  # Requires tmpfs for /tmp
#     allowPrivilegeEscalation: false
# =============================================================================
