# RouteIQ v0.1.0 Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Ship RouteIQ v0.1.0 across 4 independent waves — foundation fixes, observability, routing intelligence, and cloud-native extensions.

**Architecture:** Each wave is a branch (`tg-v010-wave{N}-*`) squash-merged to `main`. Each task within a wave is a TDD cycle: write failing test, implement, verify, commit. Waves are independent and can be parallelized by separate agents.

**Tech Stack:** Python 3.14+, FastAPI, LiteLLM v1.81.3, OpenTelemetry, Pydantic v2, pytest + hypothesis, bash (entrypoint)

---

## Wave 1: Foundation

> Branch: `tg-v010-wave1-foundation`

### Task 1: Update LiteLLM submodule to v1.81.3-stable.opus-4-6

**Files:**
- Modify: `reference/litellm` (git submodule)

**Step 1: Update submodule ref**

```bash
cd /Users/baladita/Documents/DevBox/RouteIQ
cd reference/litellm
git fetch --tags origin
git checkout v1.81.3-stable.opus-4-6
cd ../..
```

**Step 2: Verify the pin in pyproject.toml is still valid**

```bash
grep litellm pyproject.toml
```

Expected: `"litellm>=1.81.3,<1.82.0"` — no change needed.

**Step 3: Run unit tests to verify no regressions**

```bash
uv run pytest tests/unit/ -x -q
```

Expected: All tests pass.

**Step 4: Commit**

```bash
git add reference/litellm
git commit -m "chore: update litellm submodule to v1.81.3-stable.opus-4-6"
```

---

### Task 2: Update LLMRouter submodule to latest main

**Files:**
- Modify: `reference/LLMRouter` (git submodule)

**Step 1: Update submodule ref**

```bash
cd /Users/baladita/Documents/DevBox/RouteIQ
cd reference/LLMRouter
git fetch origin
git checkout origin/main
cd ../..
```

**Step 2: Run unit tests**

```bash
uv run pytest tests/unit/ -x -q
```

Expected: All tests pass (changes are README/ComfyUI only).

**Step 3: Commit**

```bash
git add reference/LLMRouter
git commit -m "chore: update LLMRouter submodule to latest main"
```

---

### Task 3: Issue #6 — Config diff engine (test first)

**Files:**
- Create: `tests/unit/test_config_diff.py`
- Modify: `src/litellm_llmrouter/config_sync.py`

**Step 1: Write the failing test**

Create `tests/unit/test_config_diff.py`:

```python
"""Tests for incremental config diffing."""

import pytest

from litellm_llmrouter.config_sync import diff_model_configs


class TestConfigDiff:
    def test_no_changes(self):
        old = [{"model_name": "gpt-4", "litellm_params": {"model": "gpt-4"}}]
        new = [{"model_name": "gpt-4", "litellm_params": {"model": "gpt-4"}}]
        result = diff_model_configs(old, new)
        assert result.added == []
        assert result.removed == []
        assert result.changed == []

    def test_added_model(self):
        old = [{"model_name": "gpt-4", "litellm_params": {"model": "gpt-4"}}]
        new = [
            {"model_name": "gpt-4", "litellm_params": {"model": "gpt-4"}},
            {"model_name": "claude-3", "litellm_params": {"model": "claude-3-sonnet"}},
        ]
        result = diff_model_configs(old, new)
        assert len(result.added) == 1
        assert result.added[0]["model_name"] == "claude-3"

    def test_removed_model(self):
        old = [
            {"model_name": "gpt-4", "litellm_params": {"model": "gpt-4"}},
            {"model_name": "claude-3", "litellm_params": {"model": "claude-3-sonnet"}},
        ]
        new = [{"model_name": "gpt-4", "litellm_params": {"model": "gpt-4"}}]
        result = diff_model_configs(old, new)
        assert len(result.removed) == 1
        assert result.removed[0]["model_name"] == "claude-3"

    def test_changed_model(self):
        old = [{"model_name": "gpt-4", "litellm_params": {"model": "gpt-4", "api_key": "old"}}]
        new = [{"model_name": "gpt-4", "litellm_params": {"model": "gpt-4", "api_key": "new"}}]
        result = diff_model_configs(old, new)
        assert result.added == []
        assert result.removed == []
        assert len(result.changed) == 1

    def test_empty_to_many(self):
        result = diff_model_configs([], [{"model_name": "m1"}, {"model_name": "m2"}])
        assert len(result.added) == 2
        assert result.removed == []

    def test_many_to_empty(self):
        result = diff_model_configs([{"model_name": "m1"}, {"model_name": "m2"}], [])
        assert result.added == []
        assert len(result.removed) == 2
```

**Step 2: Run test to verify it fails**

```bash
uv run pytest tests/unit/test_config_diff.py -v
```

Expected: FAIL with `ImportError: cannot import name 'diff_model_configs'`

**Step 3: Implement diff_model_configs in config_sync.py**

Add to `src/litellm_llmrouter/config_sync.py` (near top, after imports):

```python
import dataclasses
from typing import Any


@dataclasses.dataclass
class ConfigDiffResult:
    """Result of diffing two model config lists."""

    added: list[dict[str, Any]]
    removed: list[dict[str, Any]]
    changed: list[dict[str, Any]]


def diff_model_configs(
    old_models: list[dict[str, Any]],
    new_models: list[dict[str, Any]],
) -> ConfigDiffResult:
    """Diff two model config lists by model_name.

    Models are matched by model_name. A model is "changed" if its
    serialized dict differs but the name is the same.
    """
    old_by_name = {m.get("model_name"): m for m in old_models}
    new_by_name = {m.get("model_name"): m for m in new_models}

    old_names = set(old_by_name.keys())
    new_names = set(new_by_name.keys())

    added = [new_by_name[n] for n in sorted(new_names - old_names)]
    removed = [old_by_name[n] for n in sorted(old_names - new_names)]
    changed = [
        new_by_name[n]
        for n in sorted(old_names & new_names)
        if old_by_name[n] != new_by_name[n]
    ]

    return ConfigDiffResult(added=added, removed=removed, changed=changed)
```

**Step 4: Run test to verify it passes**

```bash
uv run pytest tests/unit/test_config_diff.py -v
```

Expected: All 6 tests PASS.

**Step 5: Commit**

```bash
git add tests/unit/test_config_diff.py src/litellm_llmrouter/config_sync.py
git commit -m "feat(#6): add config diff engine for incremental reload"
```

---

### Task 4: Issue #6 — Incremental reload with atomic swap (test first)

**Files:**
- Modify: `tests/unit/test_config_diff.py` (add reload tests)
- Modify: `src/litellm_llmrouter/config_sync.py` (add incremental_reload method)

**Step 1: Write the failing test**

Append to `tests/unit/test_config_diff.py`:

```python
class TestIncrementalReload:
    def test_incremental_reload_calls_diff(self, monkeypatch):
        """Incremental reload uses diff to identify changes."""
        from litellm_llmrouter.config_sync import ConfigSyncManager

        manager = ConfigSyncManager.__new__(ConfigSyncManager)
        manager._current_model_configs = [{"model_name": "gpt-4"}]

        diff_called = {}

        def fake_diff(old, new):
            diff_called["old"] = old
            diff_called["new"] = new
            return ConfigDiffResult(added=[], removed=[], changed=[])

        monkeypatch.setattr("litellm_llmrouter.config_sync.diff_model_configs", fake_diff)

        new_configs = [{"model_name": "gpt-4"}, {"model_name": "claude-3"}]
        manager._compute_reload_plan(new_configs)

        assert diff_called["old"] == [{"model_name": "gpt-4"}]
        assert diff_called["new"] == new_configs

    def test_rollback_on_failure(self):
        """If client init fails, old config is preserved."""
        from litellm_llmrouter.config_sync import ConfigSyncManager

        manager = ConfigSyncManager.__new__(ConfigSyncManager)
        manager._current_model_configs = [{"model_name": "gpt-4"}]

        # Simulate a reload that raises
        original = list(manager._current_model_configs)
        try:
            raise RuntimeError("Client init failed")
        except RuntimeError:
            pass

        # Old config should be unchanged
        assert manager._current_model_configs == original
```

**Step 2: Run test to verify it fails**

```bash
uv run pytest tests/unit/test_config_diff.py::TestIncrementalReload -v
```

Expected: FAIL — `_current_model_configs` and `_compute_reload_plan` don't exist yet.

**Step 3: Add _current_model_configs and _compute_reload_plan to ConfigSyncManager**

In `src/litellm_llmrouter/config_sync.py`, add to `ConfigSyncManager.__init__`:

```python
self._current_model_configs: list[dict[str, Any]] = []
```

And add the method:

```python
def _compute_reload_plan(
    self, new_model_configs: list[dict[str, Any]]
) -> ConfigDiffResult:
    """Compute what needs to change for an incremental reload."""
    return diff_model_configs(self._current_model_configs, new_model_configs)
```

**Step 4: Run test to verify it passes**

```bash
uv run pytest tests/unit/test_config_diff.py -v
```

Expected: All tests PASS.

**Step 5: Commit**

```bash
git add tests/unit/test_config_diff.py src/litellm_llmrouter/config_sync.py
git commit -m "feat(#6): add incremental reload plan computation"
```

---

### Task 5: Issue #9 — Add migration retry to entrypoint.sh

**Files:**
- Modify: `docker/entrypoint.sh`

**Step 1: Replace the raw `prisma db push` block with retry logic**

In `docker/entrypoint.sh`, replace the migration section (lines 55-62) with:

```bash
        # Only run migrations if explicitly enabled
        if [ "${LITELLM_RUN_DB_MIGRATIONS:-false}" = "true" ]; then
            if [ "${DB_MIGRATION_SKIP:-false}" = "true" ]; then
                echo "   ℹ️  DB_MIGRATION_SKIP=true - skipping migrations (replica mode)"
            else
                echo "   ⚠️  LITELLM_RUN_DB_MIGRATIONS=true - running migrations (use with caution in HA!)"

                MAX_RETRIES=${DB_MIGRATION_MAX_RETRIES:-10}
                RETRY_DELAY=${DB_MIGRATION_RETRY_DELAY:-5}
                MIGRATION_SUCCESS=false

                for i in $(seq 1 $MAX_RETRIES); do
                    echo "   Attempting database migration (attempt $i/$MAX_RETRIES)..."
                    if prisma db push --schema="$SCHEMA_PATH" --accept-data-loss 2>&1; then
                        echo "   ✅ Database migration successful."
                        MIGRATION_SUCCESS=true
                        break
                    fi
                    if [ "$i" -eq "$MAX_RETRIES" ]; then
                        echo "   ❌ Database migration failed after $MAX_RETRIES attempts."
                        exit 1
                    fi
                    SLEEP_TIME=$((RETRY_DELAY * i))
                    echo "   Migration failed. Retrying in ${SLEEP_TIME}s..."
                    sleep $SLEEP_TIME
                done
            fi
        else
            echo "   ℹ️  Skipping migrations (LITELLM_RUN_DB_MIGRATIONS not set)"
            echo "      For HA deployments, run migrations via a separate init job or leader election"
        fi
```

**Step 2: Verify entrypoint syntax**

```bash
bash -n docker/entrypoint.sh
```

Expected: No output (valid syntax).

**Step 3: Commit**

```bash
git add docker/entrypoint.sh
git commit -m "fix(#9): add exponential backoff retry for Prisma migrations"
```

---

## Wave 2: Observability & Operations

> Branch: `tg-v010-wave2-observability`

### Task 6: Issue #13 — Configurable OTEL namespace (test first)

**Files:**
- Modify: `tests/unit/test_observability.py`
- Modify: `src/litellm_llmrouter/observability.py`

**Step 1: Write the failing test**

Add to `tests/unit/test_observability.py`:

```python
def test_custom_metrics_namespace(monkeypatch):
    """ROUTEIQ_METRICS_NAMESPACE configures OTel resource."""
    monkeypatch.setenv("ROUTEIQ_METRICS_NAMESPACE", "RouteIQ/prod")
    monkeypatch.setenv("ROUTEIQ_SERVICE_NAME", "routeiq-prod")
    monkeypatch.setenv("ROUTEIQ_DEPLOYMENT_ENV", "production")

    from litellm_llmrouter.observability import get_routeiq_resource_attributes

    attrs = get_routeiq_resource_attributes()
    assert attrs["service.name"] == "routeiq-prod"
    assert attrs["deployment.environment"] == "production"
    assert attrs["routeiq.metrics.namespace"] == "RouteIQ/prod"


def test_default_metrics_namespace():
    """Defaults work when env vars are not set."""
    from litellm_llmrouter.observability import get_routeiq_resource_attributes

    attrs = get_routeiq_resource_attributes()
    assert attrs["service.name"] == "routeiq"
    assert attrs["deployment.environment"] == "default"
    assert attrs["routeiq.metrics.namespace"] == "RouteIQ"
```

**Step 2: Run test to verify it fails**

```bash
uv run pytest tests/unit/test_observability.py::test_custom_metrics_namespace -v
```

Expected: FAIL — `get_routeiq_resource_attributes` doesn't exist.

**Step 3: Implement get_routeiq_resource_attributes**

Add to `src/litellm_llmrouter/observability.py`:

```python
def get_routeiq_resource_attributes() -> dict[str, str]:
    """Return OTel resource attributes from RouteIQ env vars."""
    return {
        "service.name": os.getenv("ROUTEIQ_SERVICE_NAME", "routeiq"),
        "deployment.environment": os.getenv("ROUTEIQ_DEPLOYMENT_ENV", "default"),
        "routeiq.metrics.namespace": os.getenv("ROUTEIQ_METRICS_NAMESPACE", "RouteIQ"),
    }
```

**Step 4: Run test to verify it passes**

```bash
uv run pytest tests/unit/test_observability.py::test_custom_metrics_namespace tests/unit/test_observability.py::test_default_metrics_namespace -v
```

Expected: PASS.

**Step 5: Wire into ObservabilityManager init (use attrs in Resource creation)**

Find where `Resource` is created in `observability.py` and add the new attributes.

**Step 6: Commit**

```bash
git add tests/unit/test_observability.py src/litellm_llmrouter/observability.py
git commit -m "feat(#13): configurable OTEL metrics namespace via env vars"
```

---

### Task 7: Issue #12 — Configurable CORS (test first)

**Files:**
- Create: `tests/unit/test_cors.py`
- Modify: `src/litellm_llmrouter/gateway/app.py`

**Step 1: Write the failing test**

Create `tests/unit/test_cors.py`:

```python
"""Tests for configurable CORS origins."""

import pytest


def test_cors_parses_comma_separated_origins(monkeypatch):
    monkeypatch.setenv("ROUTEIQ_CORS_ORIGINS", "https://a.com,https://b.com")
    from litellm_llmrouter.gateway.app import _parse_cors_origins

    origins = _parse_cors_origins()
    assert origins == ["https://a.com", "https://b.com"]


def test_cors_default_wildcard():
    from litellm_llmrouter.gateway.app import _parse_cors_origins

    origins = _parse_cors_origins()
    assert origins == ["*"]


def test_cors_credentials_flag(monkeypatch):
    monkeypatch.setenv("ROUTEIQ_CORS_CREDENTIALS", "true")
    from litellm_llmrouter.gateway.app import _parse_cors_credentials

    assert _parse_cors_credentials() is True


def test_cors_credentials_default():
    from litellm_llmrouter.gateway.app import _parse_cors_credentials

    assert _parse_cors_credentials() is False
```

**Step 2: Run test to verify it fails**

```bash
uv run pytest tests/unit/test_cors.py -v
```

Expected: FAIL — functions don't exist.

**Step 3: Implement in gateway/app.py**

Add to `src/litellm_llmrouter/gateway/app.py`:

```python
import os

def _parse_cors_origins() -> list[str]:
    """Parse ROUTEIQ_CORS_ORIGINS into a list of allowed origins."""
    raw = os.getenv("ROUTEIQ_CORS_ORIGINS", "*")
    return [o.strip() for o in raw.split(",")]


def _parse_cors_credentials() -> bool:
    """Parse ROUTEIQ_CORS_CREDENTIALS flag."""
    return os.getenv("ROUTEIQ_CORS_CREDENTIALS", "false").lower() == "true"
```

**Step 4: Run test, verify pass**

```bash
uv run pytest tests/unit/test_cors.py -v
```

**Step 5: Wire CORSMiddleware into _configure_middleware**

In `_configure_middleware`, add:

```python
from starlette.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=_parse_cors_origins(),
    allow_credentials=_parse_cors_credentials(),
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Step 6: Run full unit tests**

```bash
uv run pytest tests/unit/ -x -q
```

**Step 7: Commit**

```bash
git add tests/unit/test_cors.py src/litellm_llmrouter/gateway/app.py
git commit -m "feat(#12): configurable CORS origins via ROUTEIQ_CORS_ORIGINS"
```

---

### Task 8: Issue #7 — /config/status endpoint (test first)

**Files:**
- Create: `tests/unit/test_config_status.py`
- Modify: `src/litellm_llmrouter/routes/config.py`
- Modify: `src/litellm_llmrouter/config_sync.py`

**Step 1: Write the failing test**

Create `tests/unit/test_config_status.py`:

```python
"""Tests for /config/status endpoint."""

import pytest
from unittest.mock import MagicMock, patch


def test_config_status_shape():
    """Status endpoint returns expected JSON shape."""
    from litellm_llmrouter.config_sync import ConfigSyncStatus, get_config_sync_status

    status = get_config_sync_status()
    assert isinstance(status, ConfigSyncStatus)
    assert hasattr(status, "config_source")
    assert hasattr(status, "sync_enabled")
    assert hasattr(status, "last_sync_success")
    assert hasattr(status, "config_version_hash")
    assert hasattr(status, "model_count")


def test_config_status_when_disabled():
    """When sync is disabled, status reflects that."""
    from litellm_llmrouter.config_sync import get_config_sync_status

    status = get_config_sync_status()
    assert status.sync_enabled is False or status.sync_enabled is True
```

**Step 2: Run test to verify it fails**

```bash
uv run pytest tests/unit/test_config_status.py -v
```

Expected: FAIL — `ConfigSyncStatus` and `get_config_sync_status` don't exist.

**Step 3: Implement ConfigSyncStatus dataclass and get_config_sync_status**

Add to `src/litellm_llmrouter/config_sync.py`:

```python
from datetime import datetime


@dataclasses.dataclass
class ConfigSyncStatus:
    """Status of the config sync system."""

    config_source: str | None
    sync_enabled: bool
    sync_interval_seconds: int
    last_sync_attempt: datetime | None
    last_sync_success: datetime | None
    last_sync_error: str | None
    config_version_hash: str | None
    model_count: int
    next_sync_at: datetime | None


def get_config_sync_status() -> ConfigSyncStatus:
    """Get current config sync status."""
    manager = get_sync_manager()
    source = None
    if manager.s3_sync_enabled:
        source = f"s3://{manager.s3_bucket}/{manager.s3_key}"
    elif manager.gcs_sync_enabled:
        source = f"gs://{manager.gcs_bucket}/{manager.gcs_key}"

    config_hash = None
    if manager._last_config_hash:
        config_hash = f"sha256:{manager._last_config_hash[:16]}"

    return ConfigSyncStatus(
        config_source=source,
        sync_enabled=manager.sync_enabled,
        sync_interval_seconds=manager.sync_interval,
        last_sync_attempt=None,  # TODO: track in manager
        last_sync_success=None,  # TODO: track in manager
        last_sync_error=None,
        config_version_hash=config_hash,
        model_count=len(getattr(manager, "_current_model_configs", [])),
        next_sync_at=None,
    )
```

**Step 4: Run test, verify pass**

```bash
uv run pytest tests/unit/test_config_status.py -v
```

**Step 5: Add the route to routes/config.py**

Add a new `GET /config/status` route on the `health_router` (unauthenticated):

```python
from ..config_sync import get_config_sync_status
from . import health_router

@health_router.get("/config/status")
async def config_status():
    """Config sync status. Unauthenticated (same as health probes)."""
    status = get_config_sync_status()
    return dataclasses.asdict(status)
```

**Step 6: Run full unit tests**

```bash
uv run pytest tests/unit/ -x -q
```

**Step 7: Commit**

```bash
git add tests/unit/test_config_status.py src/litellm_llmrouter/routes/config.py src/litellm_llmrouter/config_sync.py
git commit -m "feat(#7): add /config/status endpoint for sync observability"
```

---

### Task 9: Issue #8 — Per-model health endpoint (test first)

**Files:**
- Create: `tests/unit/test_model_health.py`
- Modify: `src/litellm_llmrouter/routes/health.py`
- Modify: `src/litellm_llmrouter/resilience.py`

**Step 1: Write the failing test**

Create `tests/unit/test_model_health.py`:

```python
"""Tests for per-model health endpoint."""

import pytest
from unittest.mock import MagicMock


def test_model_health_summary():
    """Model health summary counts healthy/degraded/unhealthy."""
    from litellm_llmrouter.resilience import compute_model_health_summary

    breakers = {
        "model-a": MagicMock(state="closed"),
        "model-b": MagicMock(state="half_open"),
        "model-c": MagicMock(state="open"),
    }
    summary = compute_model_health_summary(breakers)
    assert summary["healthy"] == 1
    assert summary["degraded"] == 1
    assert summary["unhealthy"] == 1
    assert summary["total"] == 3


def test_model_health_empty():
    """No breakers means all-zero summary."""
    from litellm_llmrouter.resilience import compute_model_health_summary

    summary = compute_model_health_summary({})
    assert summary == {"healthy": 0, "degraded": 0, "unhealthy": 0, "total": 0}
```

**Step 2: Run test to verify it fails**

```bash
uv run pytest tests/unit/test_model_health.py -v
```

Expected: FAIL — `compute_model_health_summary` doesn't exist.

**Step 3: Implement in resilience.py**

Add to `src/litellm_llmrouter/resilience.py`:

```python
def compute_model_health_summary(
    breakers: dict[str, Any],
) -> dict[str, int]:
    """Compute model health summary from circuit breaker states."""
    healthy = degraded = unhealthy = 0
    for breaker in breakers.values():
        state = getattr(breaker, "state", "closed")
        if state == "closed":
            healthy += 1
        elif state == "half_open":
            degraded += 1
        else:
            unhealthy += 1
    return {
        "healthy": healthy,
        "degraded": degraded,
        "unhealthy": unhealthy,
        "total": healthy + degraded + unhealthy,
    }
```

**Step 4: Run test, verify pass**

```bash
uv run pytest tests/unit/test_model_health.py -v
```

**Step 5: Add model summary to /_health/ready and new /_health/models route**

In `src/litellm_llmrouter/routes/health.py`, extend the readiness probe to include the model summary, and add a new `/_health/models` endpoint with per-model detail.

**Step 6: Run full unit tests**

```bash
uv run pytest tests/unit/ -x -q
```

**Step 7: Commit**

```bash
git add tests/unit/test_model_health.py src/litellm_llmrouter/routes/health.py src/litellm_llmrouter/resilience.py
git commit -m "feat(#8): per-model circuit breaker health in /_health/ready"
```

---

## Wave 3: Routing Intelligence

> Branch: `tg-v010-wave3-routing`

### Task 10: Strategy integration audit with latest LLMRouter

**Files:**
- Create: `tests/unit/test_strategy_compat.py`
- Modify: `src/litellm_llmrouter/strategies.py` (if API changes found)

**Step 1: Write compatibility tests**

Create `tests/unit/test_strategy_compat.py`:

```python
"""Verify all registered strategies can be instantiated with defaults."""

import pytest
from litellm_llmrouter.strategies import LLMROUTER_STRATEGIES, DEFAULT_ROUTER_HPARAMS


@pytest.mark.parametrize("strategy", LLMROUTER_STRATEGIES)
def test_strategy_is_registered(strategy):
    """Each strategy name in LLMROUTER_STRATEGIES is valid."""
    assert strategy.startswith("llmrouter-")
    short_name = strategy.replace("llmrouter-", "")
    # Custom and cost-aware are RouteIQ-native, not in LLMRouter
    if short_name not in ("custom", "cost-aware"):
        assert short_name in DEFAULT_ROUTER_HPARAMS or short_name in (
            "r1", "smallest", "largest"
        )
```

**Step 2: Run test**

```bash
uv run pytest tests/unit/test_strategy_compat.py -v
```

**Step 3: Fix any failures from upstream API changes**

**Step 4: Commit**

```bash
git add tests/unit/test_strategy_compat.py
git commit -m "test: add strategy compatibility audit for LLMRouter upstream"
```

---

### Task 11: Strategy comparison endpoint

**Files:**
- Create: `tests/unit/test_strategy_compare.py`
- Modify: `src/litellm_llmrouter/routes/` (new endpoint)
- Modify: `src/litellm_llmrouter/strategy_registry.py` (comparison data)

**Step 1: Write the failing test for comparison data collection**

**Step 2: Implement comparison data collector in strategy_registry.py**

**Step 3: Add GET /llmrouter/strategies/compare route**

**Step 4: Run tests, commit**

```bash
git commit -m "feat: add strategy comparison endpoint"
```

---

### Task 12: Cost-aware routing metric emission

**Files:**
- Modify: `tests/unit/test_cost_aware_routing.py`
- Modify: `src/litellm_llmrouter/strategies.py`

**Step 1: Write test for cost metric emission**

**Step 2: Add routeiq.routing.cost_per_1k_tokens gauge**

**Step 3: Run tests, commit**

```bash
git commit -m "feat: emit cost-per-token OTel metric for cost-aware routing"
```

---

## Wave 4: Cloud-Native Extensions

> Branch: `tg-v010-wave4-cloud-native`

### Task 13: Issue #11 — Bedrock model discovery module (test first)

**Files:**
- Create: `src/litellm_llmrouter/discovery.py`
- Create: `tests/unit/test_discovery.py`
- Modify: `src/litellm_llmrouter/routes/models.py` (or create)

**Step 1: Write failing test for discovery with mocked boto3**

```python
"""Tests for multi-account Bedrock model discovery."""

import pytest
from unittest.mock import MagicMock, patch


def test_discover_bedrock_models_mocked():
    from litellm_llmrouter.discovery import discover_bedrock_models

    mock_client = MagicMock()
    mock_client.list_foundation_models.return_value = {
        "modelSummaries": [
            {"modelId": "anthropic.claude-sonnet-4-20250514-v1:0", "modelName": "Claude Sonnet 4"},
            {"modelId": "amazon.titan-text-express-v1", "modelName": "Titan Text Express"},
        ]
    }

    with patch("boto3.client", return_value=mock_client):
        models = discover_bedrock_models(
            account_id="111111111111",
            region="us-east-1",
            role_arn=None,
        )

    assert len(models) == 2
    assert models[0]["model_id"] == "anthropic.claude-sonnet-4-20250514-v1:0"
```

**Step 2: Implement discover_bedrock_models**

**Step 3: Add /models/discover route**

**Step 4: Run tests, commit**

```bash
git commit -m "feat(#11): multi-account Bedrock model discovery"
```

---

### Task 14: Issue #1 — Cloudflare agent skills discovery

**Files:**
- Create: `tests/unit/test_agent_discovery.py`
- Modify: `src/litellm_llmrouter/routes/health.py` (add /.well-known/agent.json)

**Step 1: Write test for agent.json shape**

**Step 2: Implement endpoint**

**Step 3: Run tests, commit**

```bash
git commit -m "feat(#1): Cloudflare agent skills discovery endpoint"
```

---

### Task 15: Issue #10 — CloudFront/ALB streaming documentation

**Files:**
- Create: `docs/deployment/cloudfront-alb.md`

**Step 1: Write the documentation**

Cover: ALB idle timeout, deregistration delay, CloudFront cache/origin policies, SSE headers, Authorization header caveat, CDK snippets.

**Step 2: Commit**

```bash
git add docs/deployment/cloudfront-alb.md
git commit -m "docs(#10): CloudFront/ALB recommended settings for streaming"
```

---

### Task 16: Version bump to 0.1.0

**Files:**
- Modify: `pyproject.toml`

**Step 1: Update version**

Change `version = "0.0.5"` to `version = "0.1.0"` in `pyproject.toml`.

**Step 2: Run all tests one final time**

```bash
uv run pytest tests/unit/ -x -q
```

**Step 3: Commit and tag**

```bash
git add pyproject.toml
git commit -m "chore: bump version to 0.1.0"
git tag v0.1.0
```

---

## Parallelization Guide

These waves can be executed by **4 parallel agents**, one per wave:

| Agent | Branch | Tasks | Dependencies |
|-------|--------|-------|-------------|
| Agent 1 | `tg-v010-wave1-foundation` | Tasks 1-5 | None (start first) |
| Agent 2 | `tg-v010-wave2-observability` | Tasks 6-9 | None (independent) |
| Agent 3 | `tg-v010-wave3-routing` | Tasks 10-12 | Task 1-2 (needs updated submodules) |
| Agent 4 | `tg-v010-wave4-cloud-native` | Tasks 13-16 | None (independent) |

**Merge order:** Wave 1 first (foundation), then Wave 2-4 in any order. Version bump (Task 16) goes last after all waves are merged.
